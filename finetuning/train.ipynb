{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T23:27:58.916678Z",
     "start_time": "2024-12-09T23:27:57.831235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from finetuning.bert_masking import BertPreTrainedModel_masking\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, BertConfig, AutoModelForCausalLM, BertPreTrainedModel\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import loralib"
   ],
   "id": "c2bb9895b2c58ab5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T23:28:09.690305Z",
     "start_time": "2024-12-09T23:27:58.927069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setting device to `cuda` if gpu exists\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# initialising the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "bert = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "# bert = BertPreTrainedModel_masking.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "\n",
    "# Define LoRA parameters\n",
    "rank = 8\n",
    "num_adapters = 2\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    '''Function for tokenizing raw texts'''\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# downloading IMDB dataset from ðŸ¤— `datasets`\n",
    "raw_datasets = load_dataset(\"imdb\")\n",
    "\n",
    "# Running tokenizing function on the raw texts\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# for simplicity I have taken only the train split\n",
    "tokenized_datasets = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))"
   ],
   "id": "65965deb538c92de",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T23:18:54.191927Z",
     "start_time": "2024-12-09T23:18:54.076626Z"
    }
   },
   "cell_type": "code",
   "source": "bert",
   "id": "774b67aa2ee965a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-1): 2 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:08:00.743541Z",
     "start_time": "2024-12-09T21:07:59.408061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now lets create the torch Dataset class\n",
    "class IMDBClassificationDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.dataset[idx]\n",
    "\n",
    "        ids = torch.tensor(d['input_ids'])\n",
    "        mask = torch.tensor(d['attention_mask'])\n",
    "        label = torch.tensor(d['label'])\n",
    "        return ids, mask, label\n",
    "    \n",
    "    \n",
    "# Preparing the dataset and the Dataloader\n",
    "dataset = IMDBClassificationDataset(tokenized_datasets)\n",
    "train_dataloader = DataLoader(dataset, shuffle=True, batch_size=8)"
   ],
   "id": "8092fa0f944bcd10",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:13:50.333792Z",
     "start_time": "2024-12-09T21:13:50.232499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adapter(nn.Linear, loralib.LoRALayer):\n",
    "    # LoRA implemented in a dense layer\n",
    "    def __init__(\n",
    "        self,\n",
    "        linear: nn.Linear,\n",
    "        in_features: int, \n",
    "        out_features: int, \n",
    "        r: int = 0, \n",
    "        lora_alpha: int = 1, \n",
    "        lora_dropout: float = 0.,\n",
    "        fan_in_fan_out: bool = False, # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        nn.Linear.__init__(self, in_features, out_features, bias=False, **kwargs)\n",
    "        self.linear = linear\n",
    "        loralib.LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "                           merge_weights=merge_weights)\n",
    "\n",
    "        self.fan_in_fan_out = fan_in_fan_out\n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n",
    "            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            # Freezing the pre-trained weight matrix\n",
    "            self.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "        if fan_in_fan_out:\n",
    "            self.weight.data = self.weight.data.transpose(0, 1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        def T(w):\n",
    "            return w.transpose(0, 1) if self.fan_in_fan_out else w\n",
    "        \n",
    "        self.linear.train(mode)\n",
    "        \n",
    "        if mode:\n",
    "            if self.merge_weights and self.merged:\n",
    "                # Make sure that the weights are not merged\n",
    "                if self.r > 0:\n",
    "                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling\n",
    "                self.merged = False\n",
    "        else:\n",
    "            if self.merge_weights and not self.merged:\n",
    "                # Merge the weights and mark it\n",
    "                if self.r > 0:\n",
    "                    self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling\n",
    "                self.merged = True   \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        def T(w):\n",
    "            return w.transpose(0, 1) if self.fan_in_fan_out else w\n",
    "        if self.r > 0 and not self.merged:\n",
    "            result = self.linear(x)            \n",
    "            result += (self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling\n",
    "            return result\n",
    "        else:\n",
    "            return self.linear(x)"
   ],
   "id": "775a3bc8f35472fd",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:24:36.181353Z",
     "start_time": "2024-12-09T21:24:36.083568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomBert(transformers.PreTrainedModel):\n",
    "    def __init__(self, bert):\n",
    "        super(CustomBert, self).__init__(config=BertConfig.from_pretrained('google/bert_uncased_L-2_H-128_A-2'))\n",
    "        self.bert = bert\n",
    "        self.l1 = nn.Linear(128, 1)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Add LoRA layers to the BERT model\n",
    "        for i, (name, module) in enumerate(self.bert.named_modules()):\n",
    "            if isinstance(module, nn.Linear) and \"encoder\" in name and \"attention\" in name:\n",
    "                idx = int(name.split(\".\")[2])\n",
    "                if \"query\" in name:\n",
    "                    self.bert.encoder.layer[idx].attention.self.query = Adapter(module, module.in_features, module.out_features, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "                elif \"key\" in name:\n",
    "                    self.bert.encoder.layer[idx].attention.self.key = Adapter(module, module.in_features, module.out_features, r=8, lora_alpha=32, lora_dropout=0.1)  \n",
    "                elif \"value\" in name:\n",
    "                    self.bert.encoder.layer[idx].attention.self.value = Adapter(module, module.in_features, module.out_features, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        '''For simplicity I have added only one linear layer, you can create any type of network you want'''\n",
    "        \n",
    "        bert_out = self.bert(sent_id, attention_mask=mask)\n",
    "        o = bert_out.last_hidden_state[:,0,:]\n",
    "        o = self.do(o)\n",
    "        o = self.relu(o)\n",
    "        o = self.l1(o)\n",
    "        o = self.sigmoid(o)\n",
    "        return o"
   ],
   "id": "357091bbe980cbd4",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:28:07.136823Z",
     "start_time": "2024-12-09T21:28:07.031267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ParallelTrainer:\n",
    "    def __init__(self, base_model, device, num_adapters=2, verbose=False):\n",
    "        self.base_model = base_model\n",
    "        self.device = device\n",
    "        self.num_adapters = num_adapters\n",
    "        \n",
    "        # Add LoRA layers to the BERT model\n",
    "        self.models = []\n",
    "        \n",
    "        for _ in range(num_adapters):\n",
    "            model = CustomBert(bert).to(device)\n",
    "            loralib.utils.mark_only_lora_as_trainable(model)\n",
    "            \n",
    "            if verbose:\n",
    "                # print the trainable parameters\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        print(name)\n",
    "                        \n",
    "            self.models.append(model)                \n",
    "    \n",
    "    def train(self, train_dataloader, num_epochs=3):\n",
    "        for model in self.models:\n",
    "            model.train()\n",
    "            criterion = torch.nn.BCELoss()\n",
    "            optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "            num_training_steps = num_epochs * len(train_dataloader)\n",
    "            lr_scheduler = get_scheduler(\n",
    "                \"linear\",\n",
    "                optimizer=optimizer,\n",
    "                num_warmup_steps=0,\n",
    "                num_training_steps=num_training_steps\n",
    "            )\n",
    "\n",
    "            for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "                for batch in train_dataloader:\n",
    "                    ids, masks, labels = batch\n",
    "                    labels = labels.type(torch.float32)\n",
    "                    o = model(ids.to(device), masks.to(device))\n",
    "                    loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "                    loss.backward()\n",
    "\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()      "
   ],
   "id": "e191d723d6212698",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:33:18.210111Z",
     "start_time": "2024-12-09T21:33:18.100964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_size_of_model(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print('model size: {:.3f}MB'.format(size_all_mb))"
   ],
   "id": "8cf1ec739ae421b1",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:28:07.943862Z",
     "start_time": "2024-12-09T21:28:07.568427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the parallel trainer\n",
    "trainer = ParallelTrainer(bert, device, num_adapters=2, verbose=True)"
   ],
   "id": "53650b14caa82254",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.encoder.layer.0.attention.self.query.lora_A\n",
      "bert.encoder.layer.0.attention.self.query.lora_B\n",
      "bert.encoder.layer.0.attention.self.key.lora_A\n",
      "bert.encoder.layer.0.attention.self.key.lora_B\n",
      "bert.encoder.layer.0.attention.self.value.lora_A\n",
      "bert.encoder.layer.0.attention.self.value.lora_B\n",
      "bert.encoder.layer.1.attention.self.query.lora_A\n",
      "bert.encoder.layer.1.attention.self.query.lora_B\n",
      "bert.encoder.layer.1.attention.self.key.lora_A\n",
      "bert.encoder.layer.1.attention.self.key.lora_B\n",
      "bert.encoder.layer.1.attention.self.value.lora_A\n",
      "bert.encoder.layer.1.attention.self.value.lora_B\n",
      "bert.encoder.layer.0.attention.self.query.lora_A\n",
      "bert.encoder.layer.0.attention.self.query.lora_B\n",
      "bert.encoder.layer.0.attention.self.key.lora_A\n",
      "bert.encoder.layer.0.attention.self.key.lora_B\n",
      "bert.encoder.layer.0.attention.self.value.lora_A\n",
      "bert.encoder.layer.0.attention.self.value.lora_B\n",
      "bert.encoder.layer.1.attention.self.query.lora_A\n",
      "bert.encoder.layer.1.attention.self.query.lora_B\n",
      "bert.encoder.layer.1.attention.self.key.lora_A\n",
      "bert.encoder.layer.1.attention.self.key.lora_B\n",
      "bert.encoder.layer.1.attention.self.value.lora_A\n",
      "bert.encoder.layer.1.attention.self.value.lora_B\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:28:16.976472Z",
     "start_time": "2024-12-09T21:28:14.594768Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train(train_dataloader, num_epochs=1)",
   "id": "2cc8a71a452616e2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:34:09.444102Z",
     "start_time": "2024-12-09T21:34:09.334310Z"
    }
   },
   "cell_type": "code",
   "source": "bert",
   "id": "f65296dec9085ce9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-1): 2 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Adapter(\n",
       "              in_features=128, out_features=128, bias=False\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (key): Adapter(\n",
       "              in_features=128, out_features=128, bias=False\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (value): Adapter(\n",
       "              in_features=128, out_features=128, bias=False\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:34:06.188034Z",
     "start_time": "2024-12-09T21:34:05.959220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# initialising model, loss and optimizer\n",
    "model = CustomBert(bert)\n",
    "model"
   ],
   "id": "464b0d15522a60ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBert(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Adapter(\n",
       "                in_features=128, out_features=128, bias=False\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (key): Adapter(\n",
       "                in_features=128, out_features=128, bias=False\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (value): Adapter(\n",
       "                in_features=128, out_features=128, bias=False\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l1): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (do): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:13:53.789234Z",
     "start_time": "2024-12-09T21:13:51.960359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# setting epochs, num_training_steps and the lr_scheduler\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# training loop\n",
    "model.train()\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    for batch in train_dataloader:\n",
    "        ids, masks, labels = batch\n",
    "        labels = labels.type(torch.float32)\n",
    "        o = model(ids.to(device), masks.to(device))\n",
    "        loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()"
   ],
   "id": "a8aaced3bb602064",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Parallel\n",
    "---\n",
    "# Masking"
   ],
   "id": "1cbc41c5dd2008d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T00:23:45.842746Z",
     "start_time": "2024-12-10T00:23:44.881891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setting device to `cuda` if gpu exists\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# initialising the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "bert = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "# bert = BertPreTrainedModel_masking.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "\n",
    "# Define LoRA parameters\n",
    "rank = 8\n",
    "num_adapters = 2"
   ],
   "id": "ed62fdabe1b22d40",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T23:28:21.923372Z",
     "start_time": "2024-12-09T23:28:12.467559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "raw_datasets = load_dataset(\"imdb\")\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Now lets create the torch Dataset class\n",
    "class IMDBClassificationDataset(Dataset):\n",
    "    def __init__(self, dataset, lora_cnt=2, id=None):\n",
    "        self.dataset = dataset\n",
    "        self.id = id\n",
    "        self.lora_cnt = lora_cnt\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.id is not None:\n",
    "            return len(self.dataset)\n",
    "        else:\n",
    "            return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.dataset[idx]\n",
    "        masking = torch.zeros(self.lora_cnt)\n",
    "        \n",
    "        if self.id is not None:\n",
    "            masking[self.id] = 1           \n",
    "        else:\n",
    "            masking[idx % self.lora_cnt] = 1\n",
    "        \n",
    "        ids = torch.tensor(d['input_ids'])\n",
    "        mask = torch.tensor(d['attention_mask'])\n",
    "        label = torch.tensor(d['label'])\n",
    "        return ids, mask, label, masking"
   ],
   "id": "79e5877946a97bb9",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T23:33:27.577630Z",
     "start_time": "2024-12-09T23:33:27.475728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Adapter(nn.Linear, loralib.LoRALayer):\n",
    "    # LoRA implemented in a dense layer\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        out_features: int, \n",
    "        r: int = 0, \n",
    "        lora_alpha: int = 1, \n",
    "        lora_dropout: float = 0.,\n",
    "        fan_in_fan_out: bool = False, # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        loralib.LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "                           merge_weights=merge_weights)\n",
    "\n",
    "        self.fan_in_fan_out = fan_in_fan_out\n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n",
    "            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            # Freezing the pre-trained weight matrix\n",
    "            self.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "        if fan_in_fan_out:\n",
    "            self.weight.data = self.weight.data.transpose(0, 1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        def T(w):\n",
    "            return w.transpose(0, 1) if self.fan_in_fan_out else w\n",
    "        \n",
    "        # Merge the weights and mark it\n",
    "        if self.r > 0:\n",
    "            self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling\n",
    "        self.merged = True       \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.r > 0 and not self.merged:          \n",
    "            result = (self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling\n",
    "            return result\n",
    "        else:\n",
    "            return None"
   ],
   "id": "1f2296f568b2c10e",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T00:23:40.432677Z",
     "start_time": "2024-12-10T00:23:40.319767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiLinear(nn.Linear, loralib.LoRALayer):\n",
    "    def __init__(self, linear, in_features, out_features, r, lora_alpha, lora_dropout, num_adapters=2, merge_weights = True, **kwargs):\n",
    "        self.num_adapters = num_adapters\n",
    "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
    "        self.linear = linear\n",
    "        loralib.LoRALayer.__init__(self, r, lora_alpha, lora_dropout, merge_weights)\n",
    "        self.adapters = nn.ModuleList(loralib.Linear(in_features, out_features, r, lora_alpha, lora_dropout, merge_weights) for _ in range(num_adapters))\n",
    "\n",
    "    def forward(self, x, masking):\n",
    "        result = None\n",
    "        \n",
    "        for i in range(self.num_adapters):\n",
    "            def T(w):\n",
    "                return w.transpose(0, 1) if self.adapters[i].fan_in_fan_out else w\n",
    "            \n",
    "            if self.adapters[i].r > 0 and not self.adapters[i].merged:\n",
    "                if result is None:\n",
    "                    result = self.linear(x)          \n",
    "                result += (self.adapters[i].lora_dropout(x) @ self.adapters[i].lora_A.transpose(0, 1) @ self.adapters[i].lora_B.transpose(0, 1)) * self.adapters[i].scaling * masking[:, i].view(-1, 1, 1)\n",
    "            else:\n",
    "                result += self.linear(x)\n",
    "                \n",
    "        return result\n",
    "\n",
    "class CustomBert(transformers.PreTrainedModel):\n",
    "    def __init__(self, bert, num_adapters=2):\n",
    "        super().__init__(config=BertConfig.from_pretrained('google/bert_uncased_L-2_H-128_A-2'))\n",
    "        self.bert = bert\n",
    "        self.l1 = nn.Linear(128, 1)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Add LoRA layers to the BERT model\n",
    "        for i, (name, module) in enumerate(self.bert.named_modules()):\n",
    "            if isinstance(module, nn.Linear) and \"encoder\" and \"attention\" in name:\n",
    "                idx = int(name.split(\".\")[2])\n",
    "                if \"query\" in name:\n",
    "                    self.bert.encoder.layer[idx].attention.self.query = MultiLinear(module,\n",
    "                        module.in_features, module.out_features, r=8, lora_alpha=32, lora_dropout=0.1, num_adapters=num_adapters\n",
    "                    )\n",
    "                    \n",
    "                    assert torch.allclose(module.weight, self.bert.encoder.layer[idx].attention.self.query.linear.weight)\n",
    "                    \n",
    "                elif \"key\" in name:\n",
    "                    self.bert.encoder.layer[idx].attention.self.key = MultiLinear(module,\n",
    "                        module.in_features, module.out_features, r=8, lora_alpha=32, lora_dropout=0.1, num_adapters=num_adapters\n",
    "                    )\n",
    "                    \n",
    "                    assert torch.allclose(module.weight, self.bert.encoder.layer[idx].attention.self.key.linear.weight)\n",
    "                elif \"value\" in name:\n",
    "                    self.bert.encoder.layer[idx].attention.self.value = MultiLinear(module,\n",
    "                        module.in_features, module.out_features, r=8, lora_alpha=32, lora_dropout=0.1, num_adapters=num_adapters\n",
    "                    )\n",
    "                    \n",
    "                    assert torch.allclose(module.weight, self.bert.encoder.layer[idx].attention.self.value.linear.weight)\n",
    "                        \n",
    "                \n",
    "                \n",
    "    def forward(self, x, mask, masking):\n",
    "        bert_out = self.bert(x, attention_mask=mask, masking=masking)\n",
    "        o = bert_out.last_hidden_state[:,0,:]\n",
    "        o = self.do(o)\n",
    "        o = self.relu(o)\n",
    "        o = self.l1(o)\n",
    "        o = self.sigmoid(o)\n",
    "        return o"
   ],
   "id": "4e1739988b94ef82",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T00:23:48.994707Z",
     "start_time": "2024-12-10T00:23:48.711998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the custom model\n",
    "model = CustomBert(bert).to(device)\n",
    "\n",
    "# Mark only LoRA parameters as trainable\n",
    "loralib.utils.mark_only_lora_as_trainable(model)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "dataset = IMDBClassificationDataset(tokenized_datasets)\n",
    "train_dataloader = DataLoader(dataset, shuffle=True, batch_size=8)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ],
   "id": "d977eef8a10619e0",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T00:23:55.556208Z",
     "start_time": "2024-12-10T00:23:50.576776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    for batch in train_dataloader:\n",
    "        ids, masks, labels, masking = batch\n",
    "        labels = labels.type(torch.float)\n",
    "        o = model(ids.to(device), masks.to(device), masking.to(device))\n",
    "\n",
    "        loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()"
   ],
   "id": "52283da3345608a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T00:11:43.221433Z",
     "start_time": "2024-12-10T00:11:42.791817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save the tokenizer and the model in `./test-model/` directory \n",
    "tokenizer.save_pretrained(\"./test-model/\")\n",
    "model.save_pretrained(\"./test-model/\", push_to_hub=False)"
   ],
   "id": "ea8f75fb27ccf953",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to save a non contiguous tensor: `bert.encoder.layer.0.attention.self.query.linear.weight` which is not allowed. It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to pack it before saving.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[91], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# save the tokenizer and the model in `./test-model/` directory \u001B[39;00m\n\u001B[0;32m      2\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39msave_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./test-model/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./test-model/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpush_to_hub\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\transformers\\modeling_utils.py:2612\u001B[0m, in \u001B[0;36mPreTrainedModel.save_pretrained\u001B[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001B[0m\n\u001B[0;32m   2608\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m shard_file, shard \u001B[38;5;129;01min\u001B[39;00m shards\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m   2609\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m safe_serialization:\n\u001B[0;32m   2610\u001B[0m         \u001B[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001B[39;00m\n\u001B[0;32m   2611\u001B[0m         \u001B[38;5;66;03m# joyfulness), but for now this enough.\u001B[39;00m\n\u001B[1;32m-> 2612\u001B[0m         \u001B[43msafe_save_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshard\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_directory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshard_file\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mformat\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2613\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2614\u001B[0m         save_function(shard, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(save_directory, shard_file))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\safetensors\\torch.py:284\u001B[0m, in \u001B[0;36msave_file\u001B[1;34m(tensors, filename, metadata)\u001B[0m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msave_file\u001B[39m(\n\u001B[0;32m    254\u001B[0m     tensors: Dict[\u001B[38;5;28mstr\u001B[39m, torch\u001B[38;5;241m.\u001B[39mTensor],\n\u001B[0;32m    255\u001B[0m     filename: Union[\u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike],\n\u001B[0;32m    256\u001B[0m     metadata: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    257\u001B[0m ):\n\u001B[0;32m    258\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    259\u001B[0m \u001B[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001B[39;00m\n\u001B[0;32m    260\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    282\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[0;32m    283\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 284\u001B[0m     serialize_file(\u001B[43m_flatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m, filename, metadata\u001B[38;5;241m=\u001B[39mmetadata)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\safetensors\\torch.py:488\u001B[0m, in \u001B[0;36m_flatten\u001B[1;34m(tensors)\u001B[0m\n\u001B[0;32m    479\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m failing:\n\u001B[0;32m    480\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    481\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfailing\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m \u001B[38;5;124m        \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 488\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m    489\u001B[0m     k: {\n\u001B[0;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(v\u001B[38;5;241m.\u001B[39mdtype)\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m],\n\u001B[0;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m: v\u001B[38;5;241m.\u001B[39mshape,\n\u001B[0;32m    492\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m: _tobytes(v, k),\n\u001B[0;32m    493\u001B[0m     }\n\u001B[0;32m    494\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m tensors\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    495\u001B[0m }\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\safetensors\\torch.py:492\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    479\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m failing:\n\u001B[0;32m    480\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    481\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfailing\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m \u001B[38;5;124m        \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m    486\u001B[0m     )\n\u001B[0;32m    488\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m    489\u001B[0m     k: {\n\u001B[0;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(v\u001B[38;5;241m.\u001B[39mdtype)\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m],\n\u001B[0;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m: v\u001B[38;5;241m.\u001B[39mshape,\n\u001B[1;32m--> 492\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43m_tobytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    493\u001B[0m     }\n\u001B[0;32m    494\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m tensors\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    495\u001B[0m }\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\safetensors\\torch.py:406\u001B[0m, in \u001B[0;36m_tobytes\u001B[1;34m(tensor, name)\u001B[0m\n\u001B[0;32m    399\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    400\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to save a sparse tensor: `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` which this library does not support.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    401\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m You can make it a dense tensor before saving with `.to_dense()` but be aware this might\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    402\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m make a much larger file than needed.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    403\u001B[0m     )\n\u001B[0;32m    405\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tensor\u001B[38;5;241m.\u001B[39mis_contiguous():\n\u001B[1;32m--> 406\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    407\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to save a non contiguous tensor: `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` which is not allowed. It either means you\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    408\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m are trying to save tensors which are reference of each other in which case it\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms recommended to save\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    409\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    410\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m pack it before saving.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    411\u001B[0m     )\n\u001B[0;32m    412\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tensor\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    413\u001B[0m     \u001B[38;5;66;03m# Moving tensor to cpu before saving\u001B[39;00m\n\u001B[0;32m    414\u001B[0m     tensor \u001B[38;5;241m=\u001B[39m tensor\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: You are trying to save a non contiguous tensor: `bert.encoder.layer.0.attention.self.query.linear.weight` which is not allowed. It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to pack it before saving."
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# as this is classification so you need to mention `text-classification` as task\n",
    "classifier = pipeline('text-classification', model='tanmoyio/test-model')\n",
    "classifier(\"This movie was superb\")\n"
   ],
   "id": "d44ff93641eb817d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
