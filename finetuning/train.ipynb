{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:01:23.305487Z",
     "start_time": "2025-01-02T15:01:18.244186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import time\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import loralib\n",
    "\n",
    "import bert_multi_lora\n",
    "from custom_model import CustomBert, LoRABert"
   ],
   "id": "c2bb9895b2c58ab5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\anaconda3\\envs\\semester_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:01:23.428325Z",
     "start_time": "2025-01-02T15:01:23.308395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ],
   "id": "f1c8fef1b7a46e9a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Masking",
   "id": "f37a65bcf9d4a78b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:01:23.720603Z",
     "start_time": "2025-01-02T15:01:23.564336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ],
   "id": "98987abe7c4dd973",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:01:24.468799Z",
     "start_time": "2025-01-02T15:01:23.731779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# initialising the model\n",
    "# bert = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "# bert = BertPreTrainedModel_masking.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "\n",
    "bert = bert_multi_lora.BertModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")"
   ],
   "id": "ed62fdabe1b22d40",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:01:24.597113Z",
     "start_time": "2025-01-02T15:01:24.482186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_size_of_model(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    return (param_size + buffer_size) / 1024**2\n",
    "\n",
    "def get_sizes(max_adapters=5):\n",
    "    sizes = []\n",
    "    sizes_full = []\n",
    "    \n",
    "    sizes.append({\"model\": \"bert\", \"size\": get_size_of_model(bert)})\n",
    "    sizes_full.append({\"model\": \"bert\", \"size\": get_size_of_model(bert)})\n",
    "    \n",
    "    for i in range(1, max_adapters + 1):\n",
    "        model_custom = CustomBert(copy.deepcopy(bert), num_adapters=i)\n",
    "        models_LoRA = [LoRABert(copy.deepcopy(bert)) for _ in range(i)]\n",
    "        \n",
    "        sizes.append({\"model\": f\"bert_{i}\", \"size\": get_size_of_model(model_custom)})\n",
    "        sizes_full.append({\"model\": f\"bert_{i}\", \"size\": sum([get_size_of_model(model) for model in models_LoRA])})\n",
    "        \n",
    "    return pd.DataFrame(sizes), pd.DataFrame(sizes_full)"
   ],
   "id": "d179861adcf48aae",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:01:27.792650Z",
     "start_time": "2025-01-02T15:01:24.605459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_custom, df_LoRA = get_sizes(5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "index = np.arange(len(df_custom))\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.5\n",
    "\n",
    "ax.bar(index, df_custom[\"size\"], bar_width, alpha=opacity, color='b', label='CustomBert')\n",
    "ax.bar(index + bar_width, df_LoRA[\"size\"], bar_width, alpha=opacity, color='r', label='LoRABert')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel(\"Size (MB)\")\n",
    "ax.set_xlabel(\"Model\")\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(df_custom[\"model\"], rotation=45)\n",
    "plt.show()"
   ],
   "id": "6ac57b59d4d304fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:01:40.644867Z",
     "start_time": "2025-01-02T15:01:40.361010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define LoRA parameters\n",
    "num_adapters = 2\n",
    "rank = 8\n",
    "alpha = 32\n",
    "\n",
    "# Initialize the custom model\n",
    "model = CustomBert(copy.deepcopy(bert), num_adapters=num_adapters, rank=rank, alpha=alpha).to(device)"
   ],
   "id": "d9b67d416c4320bb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:01:42.920326Z",
     "start_time": "2025-01-02T15:01:42.810798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IMDBClassificationDataset(Dataset):\n",
    "    def __init__(self, datasets, lora_cnt=2, id=None):\n",
    "        self.datasets = datasets\n",
    "        self.id = id\n",
    "        self.lora_cnt = lora_cnt\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.id is not None:\n",
    "            return len(self.datasets[0])\n",
    "        else:\n",
    "            return sum([len(d) for d in self.datasets])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        masking = torch.zeros(self.lora_cnt)\n",
    "        \n",
    "        if self.id is not None:\n",
    "            masking[self.id] = 1\n",
    "            d = self.datasets[0][idx]\n",
    "        else:\n",
    "            masking[idx % self.lora_cnt] = 1\n",
    "            d = self.datasets[idx % self.lora_cnt][idx // self.lora_cnt]\n",
    "        \n",
    "        ids = torch.tensor(d['input_ids'])\n",
    "        mask = torch.tensor(d['attention_mask'])\n",
    "        label = torch.tensor(d['label'])\n",
    "        \n",
    "        return ids, mask, label, masking"
   ],
   "id": "4fe74418a8e100b6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:01:53.206362Z",
     "start_time": "2025-01-02T15:01:52.490142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "\n",
    "def tokenize_imdb(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def tokenize_sst2(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def tokenize_amazon(examples):\n",
    "    return tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def get_tokenized_datasets(paths, functions, split=\"train\", batched=True, num_samples=1000):\n",
    "    datasets = [load_dataset(path) for path in paths]\n",
    "    tokenized_datasets = [dataset.map(function, batched=batched)[split].shuffle(seed=42).select(range(num_samples)) for dataset, function in zip(datasets, functions)]\n",
    "    return tokenized_datasets\n",
    "\n",
    "def get_dataloaders(tokenized_datasets, lora_cnt, split=\"train\", batch_size=8):\n",
    "    if split==\"train\":\n",
    "        dataset = IMDBClassificationDataset(tokenized_datasets, lora_cnt=lora_cnt)\n",
    "        return [DataLoader(dataset, shuffle=True, batch_size=batch_size)]\n",
    "    elif split==\"test\":\n",
    "        datasets = [IMDBClassificationDataset([tokenized_datasets[i]], lora_cnt=lora_cnt, id=i) for i in range(lora_cnt)]\n",
    "        return [DataLoader(dataset, shuffle=False, batch_size=batch_size) for dataset in datasets]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid split\")"
   ],
   "id": "d49b3bd8a75d6467",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:08:36.078250Z",
     "start_time": "2025-01-02T15:04:02.276402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_test = get_tokenized_datasets([\"imdb\", \"fancyzhx/amazon_polarity\"], [tokenize_imdb, tokenize_amazon], num_samples=1000, split=\"test\")\n",
    "\n",
    "test_loaders = get_dataloaders(tokenized_test, num_adapters, split=\"test\")"
   ],
   "id": "845d0b93966bbf02",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3600000/3600000 [04:24<00:00, 13618.05 examples/s]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:09:05.845878Z",
     "start_time": "2025-01-02T15:09:03.775696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate both adapters on respective datasets\n",
    "model.eval()\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "for i in range(num_adapters):\n",
    "    total_loss = 0\n",
    "    for batch in test_loaders[i]:\n",
    "        ids, masks, labels, masking = batch\n",
    "        labels = labels.type(torch.float)\n",
    "        o = model(ids.to(device), masks.to(device), masking.to(device))\n",
    "        loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Adapter {i} loss: {total_loss / len(test_loaders[i])}\")"
   ],
   "id": "b8182de951464ffa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter 0 loss: 0.7168739409446716\n",
      "Adapter 1 loss: 0.7210031270980835\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:09:18.547971Z",
     "start_time": "2025-01-02T15:09:10.050176Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_datasets = get_tokenized_datasets([\"imdb\", \"fancyzhx/amazon_polarity\"], [tokenize_imdb, tokenize_amazon], num_samples=1000, split=\"train\")",
   "id": "73b09760f777e380",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:09:38.709120Z",
     "start_time": "2025-01-02T15:09:38.412503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the custom model\n",
    "model = CustomBert(copy.deepcopy(bert), num_adapters=num_adapters).to(device)\n",
    "\n",
    "# Mark only LoRA parameters as trainable\n",
    "loralib.utils.mark_only_lora_as_trainable(model)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "train_dataloaders = get_dataloaders(tokenized_datasets, num_adapters, split=\"test\")\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps * num_adapters\n",
    ")"
   ],
   "id": "2e6a22fd947314db",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:20:37.273062Z",
     "start_time": "2025-01-02T15:20:37.147618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lora_params_dict = {}\n",
    "\n",
    "for i in range(num_adapters):\n",
    "    lora_params_dict[f\"adapters.{i}\"] = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "    if \"adapters\" in name:\n",
    "        lora_params_dict[str.join(\".\", name.split(\".\")[-3:-1])].append(param)"
   ],
   "id": "16d422ee17e123d2",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:23:00.609052Z",
     "start_time": "2025-01-02T15:23:00.492292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# adapter_params = lora_params_dict[\"adapters.1\"]\n",
    "# for param in adapter_params:\n",
    "#     print(param.requires_grad)\n",
    "#     \n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.requires_grad)"
   ],
   "id": "f68f7f134a625b75",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:21:25.609376Z",
     "start_time": "2025-01-02T15:20:56.841303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "for i, train_dataloader in enumerate(train_dataloaders):\n",
    "    adapter_params = lora_params_dict[f\"adapters.{i}\"]\n",
    "    \n",
    "    for param in adapter_params:\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "        for batch in train_dataloader:\n",
    "            ids, masks, labels, masking = batch\n",
    "            labels = labels.type(torch.float)\n",
    "            o = model(ids.to(device), masks.to(device), masking.to(device))\n",
    "    \n",
    "            loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "            loss.backward()\n",
    "    \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "    for param in adapter_params:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "print(f\"Training time: {time.time() - start}\")"
   ],
   "id": "c7d9f0847d3ee7b9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.49s/it]\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 28.654502153396606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:21:37.497879Z",
     "start_time": "2025-01-02T15:21:29.850835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_test = get_tokenized_datasets([\"imdb\", \"jahjinx/IMDb_movie_reviews\"], [tokenize_imdb, tokenize_imdb], num_samples=1000, split=\"test\")\n",
    "\n",
    "test_loaders = get_dataloaders(tokenized_test, num_adapters, split=\"test\")"
   ],
   "id": "88dc603910a93968",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:21:41.019984Z",
     "start_time": "2025-01-02T15:21:39.050690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate both adapters on respective datasets\n",
    "model.eval()\n",
    "\n",
    "for i in range(num_adapters):\n",
    "    total_loss = 0\n",
    "    for batch in test_loaders[i]:\n",
    "        ids, masks, labels, masking = batch\n",
    "        labels = labels.type(torch.float)\n",
    "        o = model(ids.to(device), masks.to(device), masking.to(device))\n",
    "        loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Adapter {i} loss: {total_loss / len(test_loaders[i])}\")"
   ],
   "id": "2a207c2e0e21ea88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter 0 loss: 0.7102987232208252\n",
      "Adapter 1 loss: 0.717850365638733\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3a76946a82bc7177"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:48:23.646884Z",
     "start_time": "2024-12-30T20:48:23.445721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the custom model\n",
    "model = CustomBert(copy.deepcopy(bert), num_adapters=num_adapters).to(device)\n",
    "\n",
    "# Mark only LoRA parameters as trainable\n",
    "loralib.utils.mark_only_lora_as_trainable(model)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "train_dataloader = get_dataloaders(tokenized_datasets, num_adapters, split=\"train\")[0]\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps * num_adapters\n",
    ")"
   ],
   "id": "d977eef8a10619e0",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:58:47.702223Z",
     "start_time": "2024-12-30T20:58:09.336834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    for batch in train_dataloader:\n",
    "        ids, masks, labels, masking = batch\n",
    "        labels = labels.type(torch.float)\n",
    "        o = model(ids.to(device), masks.to(device), masking.to(device))\n",
    "\n",
    "        losses = []\n",
    "        for i in range(num_adapters):\n",
    "            o_adapter = o.view(-1, num_adapters).t()[i]\n",
    "            labels_adapter = labels.view(-1, num_adapters).t()[i]\n",
    "            loss = criterion(o_adapter, labels_adapter.to(device))\n",
    "            losses.append(loss)\n",
    "            \n",
    "        for i in range(num_adapters):\n",
    "            losses[i].backward(retain_graph=True)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "print(f\"Training time: {time.time() - start}\")"
   ],
   "id": "52283da3345608a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:38<00:00,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 38.35575199127197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T22:14:43.467887Z",
     "start_time": "2024-12-30T22:14:41.374157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate both adapters on respective datasets\n",
    "model.eval()\n",
    "\n",
    "for i in range(num_adapters):\n",
    "    total_loss = 0\n",
    "    for batch in test_loaders[i]:\n",
    "        ids, masks, labels, masking = batch\n",
    "        labels = labels.type(torch.float)\n",
    "        o = model(ids.to(device), masks.to(device), masking.to(device))\n",
    "        loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Adapter {i} loss: {total_loss / len(test_loaders[i])}\")"
   ],
   "id": "988f782d93723719",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter 0 loss: 0.6983270411491393\n",
      "Adapter 1 loss: 0.7010941162109375\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T18:06:12.755485Z",
     "start_time": "2024-12-29T18:06:05.112366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_normal = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "loraBert = LoRABert(copy.deepcopy(bert_normal)).to(device)\n",
    "\n",
    "# Mark only LoRA parameters as trainable\n",
    "loralib.utils.mark_only_lora_as_trainable(loraBert)\n",
    "\n",
    "# Training loop\n",
    "loraBert.train()\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "tokenized_test = get_tokenized_datasets([\"imdb\", \"jahjinx/IMDb_movie_reviews\"], [tokenize_imdb, tokenize_imdb], num_samples=1000, split=\"train\")\n",
    "test_loaders = get_dataloaders(tokenized_test, num_adapters, split=\"test\")\n",
    "\n",
    "num_epochs = 100"
   ],
   "id": "5e7c93db0d7c0407",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T18:09:26.910663Z",
     "start_time": "2024-12-29T18:06:15.361998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "for i in range(num_adapters):\n",
    "    optimizer = AdamW(loraBert.parameters(), lr=5e-5)\n",
    "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "        for batch in test_loaders[i]:\n",
    "            ids, masks, labels, _ = batch\n",
    "            labels = labels.type(torch.float)\n",
    "            o = loraBert(ids.to(device), masks.to(device))\n",
    "\n",
    "            loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "print(f\"Training time: {time.time() - start}\")"
   ],
   "id": "63b57563e03812a5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:31<00:00,  1.09it/s]\n",
      "100%|██████████| 100/100 [01:40<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 191.54238629341125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T18:09:57.281204Z",
     "start_time": "2024-12-29T18:09:57.202093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save the tokenizer and the model in `./test-model/` directory \n",
    "tokenizer.save_pretrained(\"./test-model/\")\n",
    "model.save_pretrained(\"./test-model/\", push_to_hub=False)"
   ],
   "id": "ea8f75fb27ccf953",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# as this is classification so you need to mention `text-classification` as task\n",
    "classifier = pipeline('text-classification', model='tanmoyio/test-model')\n",
    "classifier(\"This movie was superb\")\n"
   ],
   "id": "d44ff93641eb817d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Masking\n",
    "---\n",
    "# Parallel"
   ],
   "id": "3c6d27b4cfe37a03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:27:40.122739Z",
     "start_time": "2024-12-10T23:27:38.768614Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "# setting device to `cuda` if gpu exists\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# initialising the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "bert = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "# bert = BertPreTrainedModel_masking.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "\n",
    "# Define LoRA parameters\n",
    "rank = 8\n",
    "num_adapters = 2"
   ],
   "id": "3244a6ed45a0debf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:27:41.903023Z",
     "start_time": "2024-12-10T23:27:40.291324Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "# initialising the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "\n",
    "def tokenize_imdb(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def tokenize_sst2(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Now lets create the torch Dataset class\n",
    "class IMDBClassificationDataset(Dataset):\n",
    "    def __init__(self, datasets, lora_cnt=2, id=None):\n",
    "        self.datasets = datasets\n",
    "        self.id = id\n",
    "        self.lora_cnt = lora_cnt\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.id is not None:\n",
    "            return len(self.datasets[self.id])\n",
    "        else:\n",
    "            return sum([len(d) for d in self.datasets])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        masking = torch.zeros(self.lora_cnt)\n",
    "        \n",
    "        if self.id is not None:\n",
    "            masking[self.id] = 1    \n",
    "            d = self.datasets[self.id][idx]\n",
    "        else:\n",
    "            masking[idx % self.lora_cnt] = 1\n",
    "            d = self.datasets[idx % self.lora_cnt][idx // self.lora_cnt]\n",
    "        \n",
    "        ids = torch.tensor(d['input_ids'])\n",
    "        mask = torch.tensor(d['attention_mask'])\n",
    "        label = torch.tensor(d['label'])\n",
    "        \n",
    "        return ids, mask, label, masking"
   ],
   "id": "1b735c04c2509057"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:27:42.202428Z",
     "start_time": "2024-12-10T23:27:42.110798Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "import concurrent\n",
    "\n",
    "\n",
    "class ParallelTrainer:\n",
    "    def __init__(self, model, device, num_adapters=2):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.num_adapters = num_adapters             \n",
    "    \n",
    "    def train_model(self, train_dataloader, num_epochs=3):\n",
    "        # model.train()\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "        num_training_steps = num_epochs * len(train_dataloader)\n",
    "        lr_scheduler = get_scheduler(\n",
    "            \"linear\",\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "\n",
    "        for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "            for batch in train_dataloader:\n",
    "                ids, masks, labels, masking = batch\n",
    "                labels = labels.type(torch.float32)\n",
    "                o = model(ids.to(device), masks.to(device), masking.to(device))\n",
    "                loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "    def train(self, train_dataloaders, num_epochs=3):\n",
    "        model.train()\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(self.train_model, dataloader, num_epochs) for dataloader in train_dataloaders]\n",
    "            concurrent.futures.wait(futures)"
   ],
   "id": "5fc08036e25166a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:27:42.517035Z",
     "start_time": "2024-12-10T23:27:42.233428Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 5,
   "source": [
    "from custom_model import CustomBert\n",
    "\n",
    "model = CustomBert(copy.deepcopy(bert), num_adapters=num_adapters).to(device)\n",
    "\n",
    "loralib.utils.mark_only_lora_as_trainable(model)\n",
    "\n",
    "# Initialize the parallel trainer\n",
    "trainer = ParallelTrainer(model, device, num_adapters=num_adapters)"
   ],
   "id": "5babd8a38058240c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:28:10.568731Z",
     "start_time": "2024-12-10T23:27:42.546891Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 6,
   "source": [
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "sst2_dataset = load_dataset(\"stanfordnlp/sst2\")\n",
    "\n",
    "tokenized_imdb = imdb_dataset.map(tokenize_imdb, batched=True)\n",
    "tokenized_sst2 = sst2_dataset.map(tokenize_sst2, batched=True)\n",
    "\n",
    "tokenized_imdb = tokenized_imdb[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "tokenized_sst2 = tokenized_sst2[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "tokenized_datasets = [tokenized_imdb, tokenized_sst2]\n",
    "\n",
    "dataset_imdb = IMDBClassificationDataset(tokenized_datasets, lora_cnt=num_adapters, id=0)\n",
    "dataset_sst2 = IMDBClassificationDataset(tokenized_datasets, lora_cnt=num_adapters, id=1)\n",
    "test_loaders = [DataLoader(dataset_imdb, shuffle=False, batch_size=8), DataLoader(dataset_sst2, shuffle=False, batch_size=8)]"
   ],
   "id": "4db71ed93cae01c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-10T23:28:10.613170Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001B[AC:\\Users\\balanton\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:442: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "execution_count": null,
   "source": "trainer.train(test_loaders, num_epochs=3)",
   "id": "8deef8f844589118"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
