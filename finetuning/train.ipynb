{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing all the necessary libraries",
   "id": "90e561b4198812d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:31:33.991374Z",
     "start_time": "2025-01-06T22:31:28.406939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# basic libraries\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "\n",
    "# libraries for the model training and dataset loading\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# LoRA library from Microsoft (https://github.com/microsoft/LoRA/tree/main)\n",
    "import loralib\n",
    "\n",
    "# files with custom Bert model and the changed file from transformers library\n",
    "import bert_multi_lora\n",
    "from custom_model import CustomBert, LoRABert"
   ],
   "id": "c2bb9895b2c58ab5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\anaconda3\\envs\\semester_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:31:33.996926Z",
     "start_time": "2025-01-06T22:31:33.993900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# uncomment the below line if you want to automatically reload the modules\n",
    "# though this will disable debugging in the notebook\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ],
   "id": "f1c8fef1b7a46e9a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Masking",
   "id": "f37a65bcf9d4a78b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:31:34.307282Z",
     "start_time": "2025-01-06T22:31:34.266553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setting device to `cuda` if gpu exists\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ],
   "id": "98987abe7c4dd973",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:31:34.980119Z",
     "start_time": "2025-01-06T22:31:34.331286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# initialising the model\n",
    "# bert = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "# bert = BertPreTrainedModel_masking.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "\n",
    "# model is initialized from the custom file since it has the masking functionality\n",
    "bert = bert_multi_lora.BertModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")"
   ],
   "id": "ed62fdabe1b22d40",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sizes comparison",
   "id": "12fb1fc39a410bf7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this section we compare the sizes of the custom implementation of the model with multiple adapters, and the [LoRA implementation from Microsoft](https://github.com/microsoft/LoRA/tree/main). The sizes are compared for the model with different number of adapters.",
   "id": "b9a737f30e56f519"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:31:34.997783Z",
     "start_time": "2025-01-06T22:31:34.992747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_size_of_model(model):\n",
    "    \"\"\"\n",
    "    Function to get the size of the model. It calculates the size by adding all the parameters and buffers of the model.\n",
    "    \n",
    "    :param model: model for which the size is to be calculated \n",
    "    :return: size of the model in MB\n",
    "    \"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    return (param_size + buffer_size) / 1024**2\n",
    "\n",
    "def get_sizes(max_adapters=5):\n",
    "    \"\"\"\n",
    "    Function to get the sizes of the models with different number of adapters.\n",
    "    \n",
    "    :param max_adapters: maximum number of adapters to be considered\n",
    "    :return: 2 dataframes: on with the size of the model with custom adapters and the other with the size of the model with LoRA adapters\n",
    "    \"\"\"\n",
    "    sizes = []\n",
    "    sizes_full = []\n",
    "    \n",
    "    sizes.append({\"model\": \"bert\", \"size\": get_size_of_model(bert)})\n",
    "    sizes_full.append({\"model\": \"bert\", \"size\": get_size_of_model(bert)})\n",
    "    \n",
    "    for i in range(1, max_adapters + 1):\n",
    "        model_custom = CustomBert(copy.deepcopy(bert), num_adapters=i)\n",
    "        models_LoRA = [LoRABert(copy.deepcopy(bert)) for _ in range(i)]\n",
    "        \n",
    "        sizes.append({\"model\": f\"{i} adapters\", \"size\": get_size_of_model(model_custom)})\n",
    "        sizes_full.append({\"model\": f\"{i} adapters\", \"size\": sum([get_size_of_model(model) for model in models_LoRA])})\n",
    "        \n",
    "    return pd.DataFrame(sizes), pd.DataFrame(sizes_full)"
   ],
   "id": "d179861adcf48aae",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:31:37.178759Z",
     "start_time": "2025-01-06T22:31:35.000986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the sizes of the models with different number of adapters (up to 5 adapters)\n",
    "df_custom, df_LoRA = get_sizes(5)\n",
    "\n",
    "def plot_sizes(df_1, df_2):\n",
    "    \"\"\"\n",
    "    Function to plot the sizes of the models with different number of adapters.\n",
    "    \n",
    "    :param df_1: dataframe with the sizes of the model with custom adapters\n",
    "    :param df_2: dataframe with the sizes of the model with LoRA adapters\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    index = np.arange(len(df_1))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    opacity = 0.5\n",
    "\n",
    "    ax.bar(index, df_1[\"size\"], bar_width, alpha=opacity, color='b', label='CustomBert')\n",
    "    ax.bar(index + bar_width, df_2[\"size\"], bar_width, alpha=opacity, color='r', label='LoRABert')\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(\"Size (MB)\")\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(df_1[\"model\"], rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# plot_sizes(df_custom, df_LoRA)"
   ],
   "id": "6ac57b59d4d304fb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see from the plot above, the size of the model with custom adapters is significantly smaller than the size of the model with LoRA adapters. This is because the custom implementation only adds the adapters to the model, while the LoRA implementation adds the full model for each adapter.",
   "id": "98e15c38f1d814e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Custom model finetuning",
   "id": "d826b612f2f73587"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset and Dataloaders",
   "id": "14326802516b9b43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:31:37.194363Z",
     "start_time": "2025-01-06T22:31:37.189462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiAdapterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for the custom dataset that will be used for training and evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, datasets, lora_cnt=2, id=None):\n",
    "        \"\"\"\n",
    "        Constructor for the class.\n",
    "        \n",
    "        :param datasets: datasets that will be used to train and evaluate respective adapters\n",
    "        :param lora_cnt: number of adapters\n",
    "        :param id: id of the adapter (& dataset) that will be used for the evaluation. If None, the dataset is used for training\n",
    "        \"\"\"\n",
    "        self.datasets = datasets\n",
    "        self.id = id\n",
    "        self.lora_cnt = lora_cnt\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Function to get the length of the dataset.\n",
    "        \n",
    "        :return: length of the dataset. If id is not None, the length of the dataset with the given id is returned, otherwise the sum of the lengths of all datasets is returned\n",
    "        \"\"\"\n",
    "        if self.id is not None:\n",
    "            return len(self.datasets[0])\n",
    "        else:\n",
    "            return sum([len(d) for d in self.datasets])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Function to get the item from the dataset.\n",
    "        \n",
    "        :param idx: index of the item\n",
    "        :return: item from the dataset. If id is not None, the item from the dataset with the given id is returned, otherwise the item from the dataset with the index (idx % lora_cnt) is returned\n",
    "        \"\"\"\n",
    "        \n",
    "        # masking is used to determine which adapter is used for the given item, it is 1 for the adapter that is used and 0 for the other adapters\n",
    "        masking = torch.zeros(self.lora_cnt)\n",
    "        \n",
    "        if self.id is not None: \n",
    "            masking[self.id] = 1\n",
    "            d = self.datasets[0][idx]\n",
    "        else:\n",
    "            masking[idx % self.lora_cnt] = 1\n",
    "            d = self.datasets[idx % self.lora_cnt][idx // self.lora_cnt]\n",
    "        \n",
    "        ids = torch.tensor(d['input_ids'])\n",
    "        mask = torch.tensor(d['attention_mask'])\n",
    "        label = torch.tensor(d['label'])\n",
    "        \n",
    "        return ids, mask, label, masking"
   ],
   "id": "4fe74418a8e100b6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:31:37.760905Z",
     "start_time": "2025-01-06T22:31:37.208244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "\n",
    "# here we define the tokenization functions for the datasets, different functions are used for different datasets as the structure of the datasets is different\n",
    "def tokenize_imdb(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def tokenize_sst2(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def tokenize_amazon(examples):\n",
    "    return tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "def get_tokenized_datasets(paths, functions, split=\"train\", batched=True, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Function to get the tokenized datasets.\n",
    "    \n",
    "    :param paths: paths to the datasets, can be local paths or the names of the datasets from the Hugging Face library\n",
    "    :param functions: functions that will be used to tokenize the datasets\n",
    "    :param split: split of the dataset that will be used, can be \"train\" or \"test\"\n",
    "    :param batched: whether the tokenization should be batched\n",
    "    :param num_samples: number of samples that will be used from each dataset\n",
    "    :return: list of tokenized datasets\n",
    "    \"\"\"\n",
    "    datasets = [load_dataset(path) for path in paths]\n",
    "    tokenized_datasets = [dataset.map(function, batched=batched)[split].shuffle(seed=42).select(range(num_samples)) for dataset, function in zip(datasets, functions)]\n",
    "    return tokenized_datasets\n",
    "\n",
    "def get_dataloaders(tokenized_datasets, lora_cnt, split=\"train\", batch_size=8):\n",
    "    \"\"\"\n",
    "    Function to get the dataloaders for the datasets.\n",
    "    \n",
    "    :param tokenized_datasets: tokenized datasets\n",
    "    :param lora_cnt: number of adapters\n",
    "    :param split: split of the dataset that will be used, can be \"train\" or \"test\"\n",
    "    :param batch_size: batch size\n",
    "    :return: list of dataloaders\n",
    "    \"\"\"\n",
    "    # if the split is \"train\", only one dataloader is returned, otherwise a list of dataloaders is returned\n",
    "    if split==\"train\":\n",
    "        dataset = MultiAdapterDataset(tokenized_datasets, lora_cnt=lora_cnt)\n",
    "        return DataLoader(dataset, shuffle=False, batch_size=batch_size)\n",
    "    elif split==\"test\":\n",
    "        datasets = [MultiAdapterDataset([tokenized_datasets[i]], lora_cnt=lora_cnt, id=i) for i in range(lora_cnt)]\n",
    "        return [DataLoader(dataset, shuffle=False, batch_size=batch_size) for dataset in datasets]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid split\")"
   ],
   "id": "d49b3bd8a75d6467",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Defining the model",
   "id": "e251b84e0f5a8d8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:31:37.777126Z",
     "start_time": "2025-01-06T22:31:37.762914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define LoRA parameters\n",
    "num_adapters = 2\n",
    "rank = 8\n",
    "alpha = 32\n",
    "\n",
    "# Initialize the custom model\n",
    "model = CustomBert(copy.deepcopy(bert), num_adapters=num_adapters, rank=rank, alpha=alpha).to(device)"
   ],
   "id": "969a7ac709b4458e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initial evaluation",
   "id": "c57745f30a538c2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:31:48.364569Z",
     "start_time": "2025-01-06T22:31:37.787676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_test = get_tokenized_datasets([\"imdb\", \"fancyzhx/amazon_polarity\"], [tokenize_imdb, tokenize_amazon], num_samples=1000, split=\"test\")\n",
    "\n",
    "test_loaders = get_dataloaders(tokenized_test, num_adapters, split=\"test\")"
   ],
   "id": "845d0b93966bbf02",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:31:51.154698Z",
     "start_time": "2025-01-06T22:31:48.377274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate both adapters on respective datasets\n",
    "model.eval()\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "for i in range(num_adapters):\n",
    "    total_loss = 0\n",
    "    for batch in test_loaders[i]:\n",
    "        ids, masks, labels, masking = batch\n",
    "        labels = labels.type(torch.float)\n",
    "        o = model(ids.to(device), masks.to(device), masking.to(device))\n",
    "        loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Adapter {i} loss: {total_loss / len(test_loaders[i])}\")"
   ],
   "id": "b8182de951464ffa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter 0 loss: 0.6991899619102478\n",
      "Adapter 1 loss: 0.7110491509437561\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "aa1b010fabc8010c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:31:59.183809Z",
     "start_time": "2025-01-06T22:31:51.210110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = get_tokenized_datasets([\"imdb\", \"fancyzhx/amazon_polarity\"], [tokenize_imdb, tokenize_amazon], num_samples=1000, split=\"train\")\n",
    "\n",
    "train_dataloader = get_dataloaders(tokenized_datasets, num_adapters, split=\"train\")"
   ],
   "id": "73b09760f777e380",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:31:59.221056Z",
     "start_time": "2025-01-06T22:31:59.216043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# Mark only LoRA parameters as trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if \"adapter\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6, weight_decay=0)  \n",
    "\n",
    "num_epochs = 100\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ],
   "id": "a9ed8f83e593f02a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:31:59.286899Z",
     "start_time": "2025-01-06T22:31:59.281567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.requires_grad)"
   ],
   "id": "64aaeb79220553c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.encoder.layer.0.attention.self.query.adapters.0.lora_A True\n",
      "bert.encoder.layer.0.attention.self.query.adapters.0.lora_B True\n",
      "bert.encoder.layer.0.attention.self.query.adapters.1.lora_A True\n",
      "bert.encoder.layer.0.attention.self.query.adapters.1.lora_B True\n",
      "bert.encoder.layer.0.attention.self.key.adapters.0.lora_A True\n",
      "bert.encoder.layer.0.attention.self.key.adapters.0.lora_B True\n",
      "bert.encoder.layer.0.attention.self.key.adapters.1.lora_A True\n",
      "bert.encoder.layer.0.attention.self.key.adapters.1.lora_B True\n",
      "bert.encoder.layer.0.attention.self.value.adapters.0.lora_A True\n",
      "bert.encoder.layer.0.attention.self.value.adapters.0.lora_B True\n",
      "bert.encoder.layer.0.attention.self.value.adapters.1.lora_A True\n",
      "bert.encoder.layer.0.attention.self.value.adapters.1.lora_B True\n",
      "bert.encoder.layer.1.attention.self.query.adapters.0.lora_A True\n",
      "bert.encoder.layer.1.attention.self.query.adapters.0.lora_B True\n",
      "bert.encoder.layer.1.attention.self.query.adapters.1.lora_A True\n",
      "bert.encoder.layer.1.attention.self.query.adapters.1.lora_B True\n",
      "bert.encoder.layer.1.attention.self.key.adapters.0.lora_A True\n",
      "bert.encoder.layer.1.attention.self.key.adapters.0.lora_B True\n",
      "bert.encoder.layer.1.attention.self.key.adapters.1.lora_A True\n",
      "bert.encoder.layer.1.attention.self.key.adapters.1.lora_B True\n",
      "bert.encoder.layer.1.attention.self.value.adapters.0.lora_A True\n",
      "bert.encoder.layer.1.attention.self.value.adapters.0.lora_B True\n",
      "bert.encoder.layer.1.attention.self.value.adapters.1.lora_A True\n",
      "bert.encoder.layer.1.attention.self.value.adapters.1.lora_B True\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:36:48.310075Z",
     "start_time": "2025-01-06T22:31:59.354324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    for batch in train_dataloader:\n",
    "        ids, masks, labels, masking = batch\n",
    "        labels = labels.type(torch.float)\n",
    " \n",
    "        o = model(ids.to(device), masks.to(device), masking.to(device)) \n",
    "        \n",
    "        loss = criterion(torch.squeeze(o), labels.to(device))                  \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()  \n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        \n",
    "print(f\"Training time: {time.time() - start}\")"
   ],
   "id": "c7d9f0847d3ee7b9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:48<00:00,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 288.95178723335266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation",
   "id": "fd56ff8227d9da28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:36:56.946723Z",
     "start_time": "2025-01-06T22:36:48.476650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_test = get_tokenized_datasets([\"imdb\", \"fancyzhx/amazon_polarity\"], [tokenize_imdb, tokenize_amazon], num_samples=1000, split=\"test\")\n",
    "\n",
    "test_loaders = get_dataloaders(tokenized_test, num_adapters, split=\"test\")"
   ],
   "id": "88dc603910a93968",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:36:59.264623Z",
     "start_time": "2025-01-06T22:36:57.041269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate both adapters on respective datasets\n",
    "model.eval()\n",
    "\n",
    "for i in range(num_adapters):\n",
    "    total_loss = 0\n",
    "    for batch in test_loaders[i]:\n",
    "        ids, masks, labels, masking = batch\n",
    "        labels = labels.type(torch.float)\n",
    "        o = model(ids.to(device), masks.to(device), masking.to(device))\n",
    "        loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Adapter {i} loss: {total_loss / len(test_loaders[i])}\")"
   ],
   "id": "2a207c2e0e21ea88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter 0 loss: 0.6687673439979553\n",
      "Adapter 1 loss: 0.6662363996505737\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Saving the model",
   "id": "f179764673983a33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:25:42.301225800Z",
     "start_time": "2025-01-03T19:34:27.969278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save the tokenizer and the model in `./test-model/` directory \n",
    "tokenizer.save_pretrained(\"./test-model/\")\n",
    "model.save_pretrained(\"./test-model/\", push_to_hub=False)"
   ],
   "id": "c8180afd14c3f9c8",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LoRA model finetuning",
   "id": "ba62b50435faa201"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:37:00.218402Z",
     "start_time": "2025-01-06T22:36:59.283431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_normal = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "loraBerts = [LoRABert(copy.deepcopy(bert_normal)).to(device) for _ in range(num_adapters)]\n",
    "\n",
    "# Mark only LoRA parameters as trainable\n",
    "loralib.utils.mark_only_lora_as_trainable(loraBerts[0])\n",
    "loralib.utils.mark_only_lora_as_trainable(loraBerts[1])"
   ],
   "id": "4dd2ebc0f50e7ede",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initial evaluation",
   "id": "46d78de3f9d75f9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:37:09.736711Z",
     "start_time": "2025-01-06T22:37:00.242295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_test = get_tokenized_datasets([\"imdb\", \"fancyzhx/amazon_polarity\"], [tokenize_imdb, tokenize_amazon], num_samples=1000, split=\"test\")\n",
    "\n",
    "test_loaders = get_dataloaders(tokenized_test, num_adapters, split=\"test\")\n",
    "\n",
    "for i in range(num_adapters):\n",
    "    loraBerts[i].eval()\n",
    "    total_loss = 0\n",
    "    for batch in test_loaders[i]:\n",
    "        ids, masks, labels, _ = batch\n",
    "        labels = labels.type(torch.float)\n",
    "        o = loraBerts[i](ids.to(device), masks.to(device))\n",
    "        loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Adapter {i} loss: {total_loss / len(test_loaders[i])}\")"
   ],
   "id": "4a7c939eaf8897b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter 0 loss: 0.700183937072754\n",
      "Adapter 1 loss: 0.7087347493171692\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "7d23dda9ce39883c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:37:17.525132Z",
     "start_time": "2025-01-06T22:37:09.753349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "loraBerts[0].train()\n",
    "loraBerts[1].train()\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "tokenized_datasets = get_tokenized_datasets([\"imdb\", \"fancyzhx/amazon_polarity\"], [tokenize_imdb, tokenize_amazon], num_samples=1000, split=\"train\")\n",
    "train_dataloaders = get_dataloaders(tokenized_datasets, num_adapters, split=\"test\")\n",
    "\n",
    "num_epochs = 100"
   ],
   "id": "5e7c93db0d7c0407",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:41:05.495839Z",
     "start_time": "2025-01-06T22:37:17.581721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "for i in range(num_adapters):\n",
    "    optimizer = AdamW(loraBerts[i].parameters(), lr=5e-6, weight_decay=0)\n",
    "    num_training_steps = num_epochs * len(train_dataloaders[i])\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    \n",
    "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "        for batch in train_dataloaders[i]:\n",
    "            ids, masks, labels, _ = batch\n",
    "            labels = labels.type(torch.float)\n",
    "            o = loraBerts[i](ids.to(device), masks.to(device))\n",
    "\n",
    "            loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step() \n",
    "            optimizer.zero_grad()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "print(f\"Training time: {time.time() - start}\")"
   ],
   "id": "63b57563e03812a5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:52<00:00,  1.12s/it]\n",
      "100%|██████████| 100/100 [01:55<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 227.91092824935913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation",
   "id": "feb1688690b914f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:41:13.549418Z",
     "start_time": "2025-01-06T22:41:05.645982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_test = get_tokenized_datasets([\"imdb\", \"fancyzhx/amazon_polarity\"], [tokenize_imdb, tokenize_amazon], num_samples=1000, split=\"test\")\n",
    "\n",
    "test_loaders = get_dataloaders(tokenized_test, num_adapters, split=\"test\")"
   ],
   "id": "22089034730605e5",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:41:15.375105Z",
     "start_time": "2025-01-06T22:41:13.599124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(num_adapters):\n",
    "    loraBerts[i].eval()\n",
    "    total_loss = 0\n",
    "    for batch in test_loaders[i]:\n",
    "        ids, masks, labels, _ = batch\n",
    "        labels = labels.type(torch.float)\n",
    "        o = loraBerts[i](ids.to(device), masks.to(device))\n",
    "        loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Adapter {i} loss: {total_loss / len(test_loaders[i])}\")"
   ],
   "id": "c3244a28ed8d827f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter 0 loss: 0.6883003811836242\n",
      "Adapter 1 loss: 0.6896303305625916\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
