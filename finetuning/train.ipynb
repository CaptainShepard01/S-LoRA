{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:35:38.687961Z",
     "start_time": "2024-12-10T23:35:35.013493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from jupyter_client.adapter import adapters\n",
    "\n",
    "from finetuning.bert_masking import BertPreTrainedModel_masking\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, BertConfig, AutoModelForCausalLM, BertPreTrainedModel\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import loralib"
   ],
   "id": "c2bb9895b2c58ab5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Masking",
   "id": "f37a65bcf9d4a78b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:35:38.795060Z",
     "start_time": "2024-12-10T23:35:38.693996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now lets create the torch Dataset class\n",
    "class IMDBClassificationDataset(Dataset):\n",
    "    def __init__(self, datasets, lora_cnt=2, id=None):\n",
    "        self.datasets = datasets\n",
    "        self.id = id\n",
    "        self.lora_cnt = lora_cnt\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.id is not None:\n",
    "            return len(self.datasets[self.id])\n",
    "        else:\n",
    "            return sum([len(d) for d in self.datasets])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        masking = torch.zeros(self.lora_cnt)\n",
    "        \n",
    "        if self.id is not None:\n",
    "            masking[self.id] = 1    \n",
    "            d = self.datasets[self.id][idx]\n",
    "        else:\n",
    "            masking[idx % self.lora_cnt] = 1\n",
    "            d = self.datasets[idx % self.lora_cnt][idx // self.lora_cnt]\n",
    "        \n",
    "        ids = torch.tensor(d['input_ids'])\n",
    "        mask = torch.tensor(d['attention_mask'])\n",
    "        label = torch.tensor(d['label'])\n",
    "        \n",
    "        return ids, mask, label, masking"
   ],
   "id": "7b09301b88f4ab73",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:35:40.640710Z",
     "start_time": "2024-12-10T23:35:38.947188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setting device to `cuda` if gpu exists\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# initialising the model\n",
    "bert = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "# bert = BertPreTrainedModel_masking.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")"
   ],
   "id": "ed62fdabe1b22d40",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:35:40.937965Z",
     "start_time": "2024-12-10T23:35:40.850987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_size_of_model(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    return (param_size + buffer_size) / 1024**2"
   ],
   "id": "d179861adcf48aae",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:35:41.059888Z",
     "start_time": "2024-12-10T23:35:40.968714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_size = get_size_of_model(bert)\n",
    "# print('model size: {:.3f}MB'.format(bert_size))"
   ],
   "id": "a8595bc1d33aa67",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:35:41.197055Z",
     "start_time": "2024-12-10T23:35:41.090928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sizes = []\n",
    "sizes_full = []\n",
    "sizes.append({\"model\": \"bert\", \"size\": bert_size})\n",
    "sizes_full.append({\"model\": \"bert\", \"size\": bert_size})"
   ],
   "id": "58ec0b0a2e87c4aa",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:35:43.362890Z",
     "start_time": "2024-12-10T23:35:41.226563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from custom_model import CustomBert, LoRABert\n",
    "\n",
    "for i in range(1, 5):\n",
    "    model_my = CustomBert(copy.deepcopy(bert), num_adapters=i)\n",
    "    models_LoRA = [LoRABert(copy.deepcopy(bert)) for _ in range(i)]\n",
    "    sizes.append({\"model\": f\"bert_{i}\", \"size\": get_size_of_model(model_my)})\n",
    "    sizes_full.append({\"model\": f\"bert_{i}\", \"size\": sum([get_size_of_model(model) for model in models_LoRA])})"
   ],
   "id": "7919a05c8d762f40",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:35:43.860861Z",
     "start_time": "2024-12-10T23:35:43.393963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_1 = pd.DataFrame(sizes)\n",
    "df_2 = pd.DataFrame(sizes_full)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "# show histogram of model sizes for each model name for both models, add opasity to the bars\n",
    "plt.bar(df_1[\"model\"], df_1[\"size\"], label=\"CustomBert\", alpha=0.5)\n",
    "plt.bar(df_2[\"model\"], df_2[\"size\"], label=\"LoRABert\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.ylabel(\"Model size (MB)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "id": "6ac57b59d4d304fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:35:44.164618Z",
     "start_time": "2024-12-10T23:35:43.907428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define LoRA parameters\n",
    "rank = 8\n",
    "num_adapters = 2\n",
    "\n",
    "from custom_model import CustomBert, LoRABert\n",
    "# Initialize the custom model\n",
    "model = CustomBert(copy.deepcopy(bert), num_adapters=num_adapters).to(device)"
   ],
   "id": "d9b67d416c4320bb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:35:44.918022Z",
     "start_time": "2024-12-10T23:35:44.194579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "\n",
    "def tokenize_imdb(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def tokenize_sst2(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def get_tokenized_datasets(paths, functions, split=\"train\", batched=True, num_samples=1000):\n",
    "    datasets = [load_dataset(path) for path in paths]\n",
    "    tokenized_datasets = [dataset.map(function, batched=batched)[split].shuffle(seed=42).select(range(num_samples)) for dataset, function in zip(datasets, functions)]\n",
    "    return tokenized_datasets\n",
    "\n",
    "def get_dataloaders(tokenized_datasets, lora_cnt, split=\"train\", batch_size=8):\n",
    "    if split==\"train\":\n",
    "        dataset = IMDBClassificationDataset(tokenized_datasets, lora_cnt=lora_cnt)\n",
    "        return [DataLoader(dataset, shuffle=True, batch_size=batch_size)]\n",
    "    elif split==\"test\":\n",
    "        datasets = [IMDBClassificationDataset(tokenized_datasets, lora_cnt=lora_cnt, id=i) for i in range(lora_cnt)]\n",
    "        return [DataLoader(dataset, shuffle=False, batch_size=batch_size) for dataset in datasets]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid split\")"
   ],
   "id": "d49b3bd8a75d6467",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:16:39.579634Z",
     "start_time": "2024-12-10T23:16:10.980622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "sst2_dataset = load_dataset(\"stanfordnlp/sst2\")\n",
    "\n",
    "tokenized_imdb = imdb_dataset.map(tokenize_imdb, batched=True)\n",
    "tokenized_sst2 = sst2_dataset.map(tokenize_sst2, batched=True)\n",
    "\n",
    "imdb_test = tokenized_imdb[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "sst2_test = tokenized_sst2[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "tokenized_test = [imdb_test, sst2_test]\n",
    "\n",
    "dataset_imdb = IMDBClassificationDataset(tokenized_test, lora_cnt=num_adapters, id=0)\n",
    "dataset_sst2 = IMDBClassificationDataset(tokenized_test, lora_cnt=num_adapters, id=1)\n",
    "test_loaders = [DataLoader(dataset_imdb, shuffle=False, batch_size=8), DataLoader(dataset_sst2, shuffle=False, batch_size=8)]"
   ],
   "id": "845d0b93966bbf02",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:16:46.716864Z",
     "start_time": "2024-12-10T23:16:44.888107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate both adapters on respective datasets\n",
    "# model.eval()\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "for i in range(num_adapters):\n",
    "    total_loss = 0\n",
    "    for batch in test_loaders[i]:\n",
    "        ids, masks, labels, masking = batch\n",
    "        labels = labels.type(torch.float)\n",
    "        o = model(ids.to(device), masks.to(device), masking.to(device))\n",
    "        loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Adapter {i} loss: {total_loss / len(test_loaders[i])}\")"
   ],
   "id": "b8182de951464ffa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:442: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter 0 loss: 0.6939388260841369\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m     o \u001B[38;5;241m=\u001B[39m model(ids\u001B[38;5;241m.\u001B[39mto(device), masks\u001B[38;5;241m.\u001B[39mto(device), masking\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[0;32m     11\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(torch\u001B[38;5;241m.\u001B[39msqueeze(o), labels\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[1;32m---> 13\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAdapter \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_loss\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mlen\u001B[39m(test_loaders[i])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:36:57.911210Z",
     "start_time": "2024-12-10T23:36:11.994569Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_datasets = get_tokenized_datasets([\"imdb\", \"stanfordnlp/sst2\"], [tokenize_imdb, tokenize_sst2], num_samples=1000, split=\"train\")",
   "id": "73b09760f777e380",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:37:06.322275Z",
     "start_time": "2024-12-10T23:37:06.028218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the custom model\n",
    "model = CustomBert(copy.deepcopy(bert), num_adapters=num_adapters).to(device)\n",
    "\n",
    "# Mark only LoRA parameters as trainable\n",
    "loralib.utils.mark_only_lora_as_trainable(model)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "train_dataloader = get_dataloaders(tokenized_datasets, num_adapters, split=\"train\")[0]\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ],
   "id": "d977eef8a10619e0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:37:34.331367Z",
     "start_time": "2024-12-10T23:37:08.818073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    for batch in train_dataloader:\n",
    "        ids, masks, labels, masking = batch\n",
    "        labels = labels.type(torch.float)\n",
    "        o = model(ids.to(device), masks.to(device), masking.to(device))\n",
    "\n",
    "        loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()"
   ],
   "id": "52283da3345608a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\balanton\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:442: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 10/10 [00:25<00:00,  2.54s/it]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T00:11:43.221433Z",
     "start_time": "2024-12-10T00:11:42.791817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save the tokenizer and the model in `./test-model/` directory \n",
    "tokenizer.save_pretrained(\"./test-model/\")\n",
    "model.save_pretrained(\"./test-model/\", push_to_hub=False)"
   ],
   "id": "ea8f75fb27ccf953",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to save a non contiguous tensor: `bert.encoder.layer.0.attention.self.query.linear.weight` which is not allowed. It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to pack it before saving.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[91], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# save the tokenizer and the model in `./test-model/` directory \u001B[39;00m\n\u001B[0;32m      2\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39msave_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./test-model/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./test-model/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpush_to_hub\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\transformers\\modeling_utils.py:2612\u001B[0m, in \u001B[0;36mPreTrainedModel.save_pretrained\u001B[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001B[0m\n\u001B[0;32m   2608\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m shard_file, shard \u001B[38;5;129;01min\u001B[39;00m shards\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m   2609\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m safe_serialization:\n\u001B[0;32m   2610\u001B[0m         \u001B[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001B[39;00m\n\u001B[0;32m   2611\u001B[0m         \u001B[38;5;66;03m# joyfulness), but for now this enough.\u001B[39;00m\n\u001B[1;32m-> 2612\u001B[0m         \u001B[43msafe_save_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshard\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_directory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshard_file\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mformat\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2613\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2614\u001B[0m         save_function(shard, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(save_directory, shard_file))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\safetensors\\torch.py:284\u001B[0m, in \u001B[0;36msave_file\u001B[1;34m(tensors, filename, metadata)\u001B[0m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msave_file\u001B[39m(\n\u001B[0;32m    254\u001B[0m     tensors: Dict[\u001B[38;5;28mstr\u001B[39m, torch\u001B[38;5;241m.\u001B[39mTensor],\n\u001B[0;32m    255\u001B[0m     filename: Union[\u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike],\n\u001B[0;32m    256\u001B[0m     metadata: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    257\u001B[0m ):\n\u001B[0;32m    258\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    259\u001B[0m \u001B[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001B[39;00m\n\u001B[0;32m    260\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    282\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[0;32m    283\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 284\u001B[0m     serialize_file(\u001B[43m_flatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m, filename, metadata\u001B[38;5;241m=\u001B[39mmetadata)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\safetensors\\torch.py:488\u001B[0m, in \u001B[0;36m_flatten\u001B[1;34m(tensors)\u001B[0m\n\u001B[0;32m    479\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m failing:\n\u001B[0;32m    480\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    481\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfailing\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m \u001B[38;5;124m        \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 488\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m    489\u001B[0m     k: {\n\u001B[0;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(v\u001B[38;5;241m.\u001B[39mdtype)\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m],\n\u001B[0;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m: v\u001B[38;5;241m.\u001B[39mshape,\n\u001B[0;32m    492\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m: _tobytes(v, k),\n\u001B[0;32m    493\u001B[0m     }\n\u001B[0;32m    494\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m tensors\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    495\u001B[0m }\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\safetensors\\torch.py:492\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    479\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m failing:\n\u001B[0;32m    480\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    481\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfailing\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m \u001B[38;5;124m        \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m    486\u001B[0m     )\n\u001B[0;32m    488\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m    489\u001B[0m     k: {\n\u001B[0;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(v\u001B[38;5;241m.\u001B[39mdtype)\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m],\n\u001B[0;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m: v\u001B[38;5;241m.\u001B[39mshape,\n\u001B[1;32m--> 492\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43m_tobytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    493\u001B[0m     }\n\u001B[0;32m    494\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m tensors\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    495\u001B[0m }\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\safetensors\\torch.py:406\u001B[0m, in \u001B[0;36m_tobytes\u001B[1;34m(tensor, name)\u001B[0m\n\u001B[0;32m    399\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    400\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to save a sparse tensor: `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` which this library does not support.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    401\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m You can make it a dense tensor before saving with `.to_dense()` but be aware this might\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    402\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m make a much larger file than needed.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    403\u001B[0m     )\n\u001B[0;32m    405\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tensor\u001B[38;5;241m.\u001B[39mis_contiguous():\n\u001B[1;32m--> 406\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    407\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to save a non contiguous tensor: `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` which is not allowed. It either means you\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    408\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m are trying to save tensors which are reference of each other in which case it\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms recommended to save\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    409\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    410\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m pack it before saving.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    411\u001B[0m     )\n\u001B[0;32m    412\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tensor\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    413\u001B[0m     \u001B[38;5;66;03m# Moving tensor to cpu before saving\u001B[39;00m\n\u001B[0;32m    414\u001B[0m     tensor \u001B[38;5;241m=\u001B[39m tensor\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: You are trying to save a non contiguous tensor: `bert.encoder.layer.0.attention.self.query.linear.weight` which is not allowed. It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to pack it before saving."
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# as this is classification so you need to mention `text-classification` as task\n",
    "classifier = pipeline('text-classification', model='tanmoyio/test-model')\n",
    "classifier(\"This movie was superb\")\n"
   ],
   "id": "d44ff93641eb817d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Masking\n",
    "---\n",
    "# Parallel"
   ],
   "id": "3c6d27b4cfe37a03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:27:40.122739Z",
     "start_time": "2024-12-10T23:27:38.768614Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "# setting device to `cuda` if gpu exists\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# initialising the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "bert = AutoModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "# bert = BertPreTrainedModel_masking.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", device_map=\"auto\")\n",
    "\n",
    "# Define LoRA parameters\n",
    "rank = 8\n",
    "num_adapters = 2"
   ],
   "id": "3244a6ed45a0debf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:27:41.903023Z",
     "start_time": "2024-12-10T23:27:40.291324Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "# initialising the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "\n",
    "def tokenize_imdb(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def tokenize_sst2(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Now lets create the torch Dataset class\n",
    "class IMDBClassificationDataset(Dataset):\n",
    "    def __init__(self, datasets, lora_cnt=2, id=None):\n",
    "        self.datasets = datasets\n",
    "        self.id = id\n",
    "        self.lora_cnt = lora_cnt\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.id is not None:\n",
    "            return len(self.datasets[self.id])\n",
    "        else:\n",
    "            return sum([len(d) for d in self.datasets])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        masking = torch.zeros(self.lora_cnt)\n",
    "        \n",
    "        if self.id is not None:\n",
    "            masking[self.id] = 1    \n",
    "            d = self.datasets[self.id][idx]\n",
    "        else:\n",
    "            masking[idx % self.lora_cnt] = 1\n",
    "            d = self.datasets[idx % self.lora_cnt][idx // self.lora_cnt]\n",
    "        \n",
    "        ids = torch.tensor(d['input_ids'])\n",
    "        mask = torch.tensor(d['attention_mask'])\n",
    "        label = torch.tensor(d['label'])\n",
    "        \n",
    "        return ids, mask, label, masking"
   ],
   "id": "1b735c04c2509057"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:27:42.202428Z",
     "start_time": "2024-12-10T23:27:42.110798Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "import concurrent\n",
    "\n",
    "\n",
    "class ParallelTrainer:\n",
    "    def __init__(self, model, device, num_adapters=2):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.num_adapters = num_adapters             \n",
    "    \n",
    "    def train_model(self, train_dataloader, num_epochs=3):\n",
    "        # model.train()\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "        num_training_steps = num_epochs * len(train_dataloader)\n",
    "        lr_scheduler = get_scheduler(\n",
    "            \"linear\",\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "\n",
    "        for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "            for batch in train_dataloader:\n",
    "                ids, masks, labels, masking = batch\n",
    "                labels = labels.type(torch.float32)\n",
    "                o = model(ids.to(device), masks.to(device), masking.to(device))\n",
    "                loss = criterion(torch.squeeze(o), labels.to(device))\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "    def train(self, train_dataloaders, num_epochs=3):\n",
    "        model.train()\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(self.train_model, dataloader, num_epochs) for dataloader in train_dataloaders]\n",
    "            concurrent.futures.wait(futures)"
   ],
   "id": "5fc08036e25166a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:27:42.517035Z",
     "start_time": "2024-12-10T23:27:42.233428Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 5,
   "source": [
    "from custom_model import CustomBert\n",
    "\n",
    "model = CustomBert(copy.deepcopy(bert), num_adapters=num_adapters).to(device)\n",
    "\n",
    "loralib.utils.mark_only_lora_as_trainable(model)\n",
    "\n",
    "# Initialize the parallel trainer\n",
    "trainer = ParallelTrainer(model, device, num_adapters=num_adapters)"
   ],
   "id": "5babd8a38058240c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:28:10.568731Z",
     "start_time": "2024-12-10T23:27:42.546891Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 6,
   "source": [
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "sst2_dataset = load_dataset(\"stanfordnlp/sst2\")\n",
    "\n",
    "tokenized_imdb = imdb_dataset.map(tokenize_imdb, batched=True)\n",
    "tokenized_sst2 = sst2_dataset.map(tokenize_sst2, batched=True)\n",
    "\n",
    "tokenized_imdb = tokenized_imdb[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "tokenized_sst2 = tokenized_sst2[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "tokenized_datasets = [tokenized_imdb, tokenized_sst2]\n",
    "\n",
    "dataset_imdb = IMDBClassificationDataset(tokenized_datasets, lora_cnt=num_adapters, id=0)\n",
    "dataset_sst2 = IMDBClassificationDataset(tokenized_datasets, lora_cnt=num_adapters, id=1)\n",
    "test_loaders = [DataLoader(dataset_imdb, shuffle=False, batch_size=8), DataLoader(dataset_sst2, shuffle=False, batch_size=8)]"
   ],
   "id": "4db71ed93cae01c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-10T23:28:10.613170Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001B[AC:\\Users\\balanton\\anaconda3\\envs\\mnlp_m2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:442: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "execution_count": null,
   "source": "trainer.train(test_loaders, num_epochs=3)",
   "id": "8deef8f844589118"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
